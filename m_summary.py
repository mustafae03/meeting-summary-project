# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qn6DC0wvE1mWlRlcdts81Ck2S2jwEAOn
"""

!pip install -q requests torch transformers sentencepiece accelerate openai
!pip install -U bitsandbytes
!pip install -U transformers accelerate

import os
import requests
from IPython.display import Markdown, display, update_display
from openai import OpenAI
from google.colab import drive
from huggingface_hub import login
from google.colab import userdata
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig
import torch

AUDIO_MODEL = "whisper-1"
LLAMA = "meta-llama/Meta-Llama-3.1-8B-Instruct"

from google.colab import drive
drive.mount('/content/drive')

audio_filename = "/content/drive/MyDrive/sandiego_c715f40bcd35aa72dde1f47ee17fecfd.mp3"

hf_token=userdata.get('hf')
login(hf_token)

open_ai_key = userdata.get('openaikey')
openai = OpenAI(api_key=open_ai_key)

with open(audio_filename, "rb") as audio_file:
    transcript = openai.audio.transcriptions.create(
        model=AUDIO_MODEL,
        file=audio_file,
        response_format="text"
    )

print(transcript)

user_prompt = "Lütfen katılımcılarla birlikte bir özet, yer ve tarih, tartışma noktaları, çıkarımlar ve sahipleriyle birlikte eylem öğeleri de dahil olmak üzere tutanakları işaretleyerek yaz ve kısa olmasın ayrıca tekrarlardan kaçın hayal görme sadece transkripteki bilgiler "
system_prompt =  "Sadece AŞAĞIDAKİ TRANSCRIPT'TEKİ ifadeleri kullanarak ÖZET çıkar. Metinde olmayan hiçbir bilgi EKLEME. Tahmin etme.  İsim/yer/tarih geçmiyorsa '—' yaz. Türkçe, maddeli ve kısa yaz."

messages = [

          { "role": "system", "content": system_prompt },
          { "role": "user", "content": user_prompt },


]

from transformers import BitsAndBytesConfig
import torch

quant_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(LLAMA)
model = AutoModelForCausalLM.from_pretrained(LLAMA, quantization_config=quant_config, device_map="auto")
streamer = TextStreamer(tokenizer)
input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=300 , streamer = streamer, temperature=0.3)
print(tokenizer.decode(output[0]))



def generate_meeting_summary(audio_transcript):

    system_prompt = "temel tartışma noktalarını ,  çıkarımlar vve eylem maddelerini toplantının genel özetini çıkaran bir yardımcı asistansın"
    user_prompt = "Lütfen katılımcılarla birlikte bir özet, yer ve tarih, tartışma noktaları, çıkarımlar ve sahipleriyle birlikte eylem öğeleri de dahil olmak üzere tutanakları işaretleyerek yaz ve kısa olmasın ayrıca tekrarlardan kaçın "


    messages = [
        { "role": "system", "content": system_prompt },
        { "role": "user", "content": user_prompt },
    ]


    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
    streamer = TextStreamer(tokenizer)

    output = model.generate(inputs=input_ids, max_new_tokens=512, streamer=streamer)
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)


    assistant_response_start = generated_text.find("<|assistant|>")
    if assistant_response_start != -1:

        return generated_text[assistant_response_start + len("<|assistant|>"):].strip()
    else:

        return generated_text.strip()

import gradio as gr


with gr.Blocks(title="Proje 3 - Tutanak  Özet") as ui:
    gr.Markdown("Transcript Tutanak\nWhisper çıktısını metin kutusuna yapıştır, butona bas.")

    with gr.Row():
        girdi = gr.Textbox(label="Transcript (metin)", lines=10, placeholder="Whisper çıktısını buraya yapıştır")
        buton = gr.Button("Özetle")

    cikti = gr.Textbox(label="Çıktı (Markdown)", lines=20)


    buton.click(
        fn=lambda t: generate_meeting_summary(t),
        inputs=girdi,
        outputs=cikti
    )

ui.launch(inbrowser=True)
